{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Hagen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Read in files\n",
    "    - Loop over ham directory and extract the email body (= Text after the first \\n)\n",
    "    - Save into a pandas data-frame with the label 0\n",
    "    - Same for spam but with label 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "root = \"./\"\n",
    "def convert_files(directory):\n",
    "    # build directory path\n",
    "    directory_path = os.path.join(root, directory)\n",
    "    \n",
    "    for mail in os.listdir(directory_path):\n",
    "        file_path = directory_path + \"/\" + mail\n",
    "        with open(file_path, \"r\", encoding='latin-1') as m:\n",
    "            mail_dict = parse_message(m)\n",
    "            yield mail_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def parse_message(msg):\n",
    "    body = ''\n",
    "    email = {}\n",
    "    email['subject'] = ''\n",
    "    in_body = False\n",
    "    exclude_terms = ['URL:', 'Date:', 'Return-Path:']\n",
    "    sw = stopwords.words(\"english\")\n",
    "    \n",
    "    for line in msg:\n",
    "        if line == '\\n':\n",
    "            in_body = True\n",
    "            continue\n",
    "            \n",
    "        if any(term in line for term in exclude_terms):\n",
    "            continue\n",
    "            \n",
    "        \n",
    "        #get rid of html markup\n",
    "        line = re.sub('<[^>]*>', '', line)\n",
    "        \n",
    "        #get rid of stopwords\n",
    "        line = ' '.join([word for word in line.split() if word.lower() not in sw])\n",
    "        \n",
    "        \n",
    "        if in_body:\n",
    "            body += line.strip()\n",
    "            email['body'] = body\n",
    "        elif line.startswith('From:'):\n",
    "            sender = line.strip()\n",
    "            sender = sender.replace('\"', '')\n",
    "            sender = line[5:]\n",
    "            email['sender'] = sender\n",
    "        elif line.startswith('Subject:'):\n",
    "            subject = line.strip()\n",
    "            subject = line[8:]\n",
    "            email['subject'] = subject\n",
    "            \n",
    "        # Optionally an else branch could extract more features\n",
    "        \n",
    "    return email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def make_df(path, label):\n",
    "    rows = []\n",
    "    index = []\n",
    "    for i, text in enumerate(convert_files(path)):\n",
    "        rows.append({'body': text['body'],'subject': text['subject'], 'sender' : text['sender'], 'label': label}) \n",
    "        index.append(i)\n",
    "    \n",
    "    df = pd.DataFrame(rows, index=index)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_ham = make_df('corpus/train-ham', 0)\n",
    "df_spam = make_df('corpus/train-spam', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create Pipeline for e-mail classification\n",
    "    - Vectorization\n",
    "    - Classifier (NaiveBayes / SVM / LogReg)\n",
    "    - (optional sp√§ter: feature selection, z.B. IG)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create test and training data\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "df_final = pd.concat([df_ham, df_spam])\n",
    "df_final = df_final.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "X = df_final[['body', 'subject', 'sender']]\n",
    "y = df_final.label.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Create ItemSelector class to be used in FeatureUnion later. (To handle multiple text-columns in X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin\n",
    "class ItemSelector(TransformerMixin):\n",
    "    def __init__(self, column):\n",
    "        self.column = column\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        return X[self.column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create Pipeline\n",
    "from sklearn.pipeline import Pipeline, make_pipeline, make_union\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# body pipe\n",
    "body_pipe = make_pipeline(\n",
    "    ItemSelector('body'),\n",
    "    TfidfVectorizer(encoding='latin1', lowercase=False, ngram_range=(1,3))\n",
    ")\n",
    "\n",
    "# subject pipe\n",
    "subject_pipe = make_pipeline(\n",
    "    ItemSelector('subject'),\n",
    "    TfidfVectorizer(encoding='latin1', lowercase=False, ngram_range=(1,3))\n",
    ")\n",
    "\n",
    "# sender pipe\n",
    "sender_pipe = make_pipeline(\n",
    "    ItemSelector('sender'),\n",
    "    TfidfVectorizer(encoding='latin1', lowercase=False, ngram_range=(1,3))\n",
    ")\n",
    "\n",
    "feature_union = make_union(body_pipe, subject_pipe, sender_pipe)\n",
    "\n",
    "lr_tfidf = Pipeline([\n",
    "    ('union', feature_union),\n",
    "    ('clf', LogisticRegression(random_state=0))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. K-fold Cross-Validation on the data\n",
    "    - create k-fold test for each classifier and train data + make predictions on validation-sets\n",
    "    - figure out best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion - LR\n",
      "[[1684   57]\n",
      " [  14 1519]]\n",
      "Mean-Accuracy LR: 0.9783145411676821\n",
      "Standard Deviation LR: 0.005052148526313248\n",
      "Variance LR: 2.5524204731929125e-05\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "k_fold = KFold(n_splits=5)\n",
    "confusion_lr = np.array([[0,0], [0,0]])\n",
    "acc_lr = []\n",
    "\n",
    "for i, (train, validate) in enumerate(k_fold.split(X_train)):\n",
    "    X_tr, X_val = X_train.values[train], X_train.values[validate]\n",
    "    y_tr, y_val = y_train[train], y_train[validate]\n",
    "    \n",
    "    X_tr = pd.DataFrame(X_tr, columns=['body', 'subject', 'sender'])\n",
    "    X_val = pd.DataFrame(X_val, columns=['body', 'subject', 'sender'])\n",
    "    \n",
    "    lr_tfidf.fit(X_tr, y_tr)\n",
    "    \n",
    "    acc_lr.append(lr_tfidf.score(X_val, y_val))\n",
    "    confusion_lr += confusion_matrix(y_val, lr_tfidf.predict(X_val))\n",
    "    \n",
    "    \n",
    "print('Confusion - LR')\n",
    "print(confusion_lr)\n",
    "acc_lr = np.asarray(acc_lr)\n",
    "print('Mean-Accuracy LR: {}'.format(np.mean(acc_lr)))\n",
    "print('Standard Deviation LR: {}'.format(np.std(acc_lr)))\n",
    "print('Variance LR: {}'.format(np.var(acc_lr)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Tune LR pipe  via GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Tune LR \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = [\n",
    "    {\n",
    "        'clf__penalty': ['l1', 'l2'],\n",
    "        'clf__C': [0.1, 1.0, 10.0, 100.0]\n",
    "    }\n",
    "]\n",
    "\n",
    "gs_lr = GridSearchCV(\n",
    "    lr_tfidf,\n",
    "    param_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "gs_lr.fit(X_train, y_train)\n",
    "print('Parameter: {}'.format(gs_lr.best_params_))\n",
    "print('Accuracy (CV): {}'.format(gs_lr.best_score_))\n",
    "best_classifier = gs_lr.best_estimator_\n",
    "print('Accuracy (Test): {}'.format(best_classifier.score(X_test, y_test)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test = make_df('corpus/test', 0)\n",
    "X = df_test[['body', 'subject', 'sender']]\n",
    "y = best_classifier.predict(X)\n",
    "my_list = y.tolist()\n",
    "my_list = ['SPAM' if elem == 1  else 'HAM' for elem in my_list]\n",
    "filenames = os.listdir('corpus/test')\n",
    "file = open('result', 'w')\n",
    "\n",
    "for i, item in enumerate(my_list):\n",
    "    file.write(\"{} \\t {}\\n\".format(filenames[i],item))\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=0, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_classifier.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result:\n",
    "    The best estimator is a logistic regression in a pipeline with a CountVectorizer \n",
    "    Optimal parameters for Count-Vec: lowercase = False, ngram_range = (1,3)\n",
    "    Optimal parameters for LR: C = 100, penalty = l2\n",
    "    Accuracy on the test-set: 0.982"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Refactor code into train  / predict script"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
