{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hagen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Read in files\n",
    "    - Loop over ham directory and extract the email body (= Text after the first \\n)\n",
    "    - Save into a pandas data-frame with the label 0\n",
    "    - Same for spam but with label 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "root = \"./\"\n",
    "def convert_files(directory):\n",
    "    # build directory path\n",
    "    directory_path = os.path.join(root, directory)\n",
    "    \n",
    "    for mail in os.listdir(directory_path):\n",
    "        file_path = directory_path + \"/\" + mail\n",
    "        with open(file_path, \"r\", encoding='latin-1') as m:\n",
    "            mail_dict = parse_message(m)\n",
    "            yield mail_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def parse_message(msg):\n",
    "    body = ''\n",
    "    email = {}\n",
    "    #email['subject'] = ''\n",
    "    in_body = False\n",
    "    exclude_terms = ['URL:', 'Date:', 'Return-Path:']\n",
    "    sw = stopwords.words(\"english\")\n",
    "    \n",
    "    for line in msg:\n",
    "        if line == '\\n':\n",
    "            in_body = True\n",
    "            continue\n",
    "            \n",
    "        if any(term in line for term in exclude_terms):\n",
    "            continue\n",
    "            \n",
    "        \n",
    "        #get rid of html markup\n",
    "        line = re.sub('<[^>]*>', '', line)\n",
    "        \n",
    "        #get rid of stopwords\n",
    "        line = ' '.join([word for word in line.split() if word.lower() not in sw])\n",
    "        \n",
    "        \n",
    "        if in_body:\n",
    "            body += line.strip()\n",
    "            email['body'] = body\n",
    "#         elif line.startswith('From:'):\n",
    "#             sender = line.strip()\n",
    "#             sender = sender.replace('\"', '')\n",
    "#             sender = line[5:]\n",
    "#             email['sender'] = sender\n",
    "#         elif line.startswith('Subject:'):\n",
    "#             subject = line.strip()\n",
    "#             subject = line[8:]\n",
    "#             email['subject'] = subject\n",
    "            \n",
    "        # Optionally an else branch could extract more features\n",
    "        \n",
    "    return email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def make_df(path, label):\n",
    "    rows = []\n",
    "    index = []\n",
    "    for i, text in enumerate(convert_files(path)):\n",
    "        rows.append({'body': text['body'], 'label': label}) #'subject': text['subject'], 'sender' : text['sender'],\n",
    "        index.append(i)\n",
    "    \n",
    "    df = pd.DataFrame(rows, index=index)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_ham = make_df('corpus/train-ham', 0)\n",
    "df_spam = make_df('corpus/train-spam', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create Pipeline for e-mail classification\n",
    "    - Vectorization\n",
    "    - Classifier (NaiveBayes / SVM / LogReg)\n",
    "    - (optional sp√§ter: feature selection, z.B. IG)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create test and training data\n",
    "df_final = pd.concat([df_ham, df_spam])\n",
    "df_final = df_final.sample(frac=1).reset_index(drop=True)\n",
    "X = df_final.body.values\n",
    "y = df_final.label.values\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#tfidf = TfidfVectorizer(strip_accents=None, lowercase=False, preprocessor=None) #use defaults for smoothing and ngrams\n",
    "\n",
    "# LogisticRegression\n",
    "lr_tfidf = Pipeline([\n",
    "    ('count', CountVectorizer(ngram_range=(1,2))),\n",
    "    ('vect', TfidfTransformer()),\n",
    "    ('clf', LogisticRegression(random_state=0)) # default C to 1.0, mb tune later\n",
    "])\n",
    "\n",
    "# Bayes\n",
    "bayes_tfidf = Pipeline([\n",
    "    ('count', CountVectorizer(ngram_range=(1,2))),\n",
    "    ('vect', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB()) # default smoothing, and settings\n",
    "])\n",
    "\n",
    "# SVM\n",
    "svm_tfidf = Pipeline([\n",
    "    ('count', CountVectorizer(ngram_range=(1,2))),\n",
    "    ('vect', TfidfTransformer()),\n",
    "    ('clf', SVC(C=1.0, gamma=1e-5)) # default C to 1.0, mb tune later\n",
    "])\n",
    "\n",
    "# RandomForest\n",
    "rf_tfidf = Pipeline([\n",
    "    ('count', CountVectorizer(ngram_range=(1,2))),\n",
    "    ('vect', TfidfTransformer()),\n",
    "    ('clf', RandomForestClassifier()) # default n-trees to 10, mb tune later\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. K-fold Cross-Validation on the data, compare different models\n",
    "    - create k-fold test for each classifier and train data + make predictions on validation-sets\n",
    "    - figure out best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9648854961832061 (Fold: 0)\n",
      "Test Accuracy: 0.9740458015267176 (Fold: 0)\n",
      "Test Accuracy: 0.49923664122137407 (Fold: 0)\n",
      "Test Accuracy: 0.9221374045801527 (Fold: 0)\n",
      "Test Accuracy: 0.9694656488549618 (Fold: 1)\n",
      "Test Accuracy: 0.9648854961832061 (Fold: 1)\n",
      "Test Accuracy: 0.4900763358778626 (Fold: 1)\n",
      "Test Accuracy: 0.9358778625954198 (Fold: 1)\n",
      "Test Accuracy: 0.966412213740458 (Fold: 2)\n",
      "Test Accuracy: 0.966412213740458 (Fold: 2)\n",
      "Test Accuracy: 0.5557251908396946 (Fold: 2)\n",
      "Test Accuracy: 0.9129770992366413 (Fold: 2)\n",
      "Test Accuracy: 0.9755725190839695 (Fold: 3)\n",
      "Test Accuracy: 0.9694656488549618 (Fold: 3)\n",
      "Test Accuracy: 0.5740458015267176 (Fold: 3)\n",
      "Test Accuracy: 0.9236641221374046 (Fold: 3)\n",
      "Test Accuracy: 0.9709480122324159 (Fold: 4)\n",
      "Test Accuracy: 0.9709480122324159 (Fold: 4)\n",
      "Test Accuracy: 0.5489296636085627 (Fold: 4)\n",
      "Test Accuracy: 0.9235474006116208 (Fold: 4)\n",
      "Confusion - LR\n",
      "[[1703   44]\n",
      " [  56 1471]]\n",
      "Confusion - Bayes\n",
      "[[1680   67]\n",
      " [  34 1493]]\n",
      "Confusion - SVM\n",
      "[[1747    0]\n",
      " [1527    0]]\n",
      "Confusion - Random Forest\n",
      "[[1572  175]\n",
      " [  75 1452]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "k_fold = KFold(n_splits=5)\n",
    "confusion_lr = np.array([[0,0], [0,0]])\n",
    "confusion_bayes = np.array([[0,0], [0,0]])\n",
    "confusion_svm = np.array([[0,0], [0,0]])\n",
    "confusion_rf = np.array([[0,0], [0,0]])\n",
    "\n",
    "for i, (train, validate) in enumerate(k_fold.split(X_train)):\n",
    "    X_tr, X_val = X_train[train], X_train[validate]\n",
    "    y_tr, y_val = y_train[train], y_train[validate]\n",
    "    \n",
    "    lr_tfidf.fit(X_tr, y_tr)\n",
    "    bayes_tfidf.fit(X_tr, y_tr)\n",
    "    svm_tfidf.fit(X_tr, y_tr)\n",
    "    rf_tfidf.fit(X_tr, y_tr)\n",
    "\n",
    "    print(\"Test Accuracy: {} (Fold: {})\".format(lr_tfidf.score(X_val, y_val), i))\n",
    "    print(\"Test Accuracy: {} (Fold: {})\".format(bayes_tfidf.score(X_val, y_val), i))\n",
    "    print(\"Test Accuracy: {} (Fold: {})\".format(svm_tfidf.score(X_val, y_val), i))\n",
    "    print(\"Test Accuracy: {} (Fold: {})\".format(rf_tfidf.score(X_val, y_val), i))\n",
    "    \n",
    "    confusion_lr += confusion_matrix(y_val, lr_tfidf.predict(X_val))\n",
    "    confusion_bayes += confusion_matrix(y_val, bayes_tfidf.predict(X_val))\n",
    "    confusion_svm += confusion_matrix(y_val, svm_tfidf.predict(X_val))\n",
    "    confusion_rf += confusion_matrix(y_val, rf_tfidf.predict(X_val))\n",
    "    \n",
    "    \n",
    "print('Confusion - LR')\n",
    "print(confusion_lr)\n",
    "print('Confusion - Bayes')\n",
    "print(confusion_bayes)\n",
    "print('Confusion - SVM')\n",
    "print(confusion_svm)\n",
    "print('Confusion - Random Forest')\n",
    "print(confusion_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# svm looks odd, kinda as if it just classifies naive --> need to check later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Tune LR model via GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: {'clf__C': 100.0, 'clf__penalty': 'l2', 'count__lowercase': False, 'count__ngram_range': (1, 3)}\n",
      "Accuracy (CV): 0.9795357361026268\n",
      "Accuracy (Test): 0.9793447293447294\n"
     ]
    }
   ],
   "source": [
    "# Tune LR \n",
    "from sklearn.grid_search import GridSearchCV\n",
    "param_grid = [\n",
    "    {\n",
    "        'count__ngram_range': [(1,2), (1,3)],\n",
    "        'count__lowercase': [True, False],\n",
    "        'clf__penalty': ['l1', 'l2'],\n",
    "        'clf__C': [0.1, 1.0, 10.0, 100.0]\n",
    "    }\n",
    "]\n",
    "\n",
    "gs_lr_tfidf = GridSearchCV(\n",
    "    lr_tfidf,\n",
    "    param_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "gs_lr_tfidf.fit(X_train, y_train)\n",
    "print('Parameter: {}'.format(gs_lr_tfidf.best_params_))\n",
    "print('Accuracy (CV): {}'.format(gs_lr_tfidf.best_score_))\n",
    "best_classifier = gs_lr_tfidf.best_estimator_\n",
    "print('Accuracy (Test): {}'.format(best_classifier.score(X_test, y_test)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result:\n",
    "    The best estimator is a logistic regression in a pipeline with a CountVectorizer & tfidf-transformer\n",
    "    Optimal parameters for Count-Vec: lowercase = False, ngram_range = (1,3)\n",
    "    Optimal parameters for LR: C = 100, penalty = l2\n",
    "    Accuracy on the test-set: 0.979"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Refactor code into train  / predict script"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
