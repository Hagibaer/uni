{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hagen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Read in files\n",
    "    - Loop over ham directory and extract the email body (= Text after the first \\n)\n",
    "    - Save into a pandas data-frame with the label 0\n",
    "    - Same for spam but with label 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "root = \"./\"\n",
    "def convert_files(directory):\n",
    "    # build directory path\n",
    "    directory_path = os.path.join(root, directory)\n",
    "    \n",
    "    for mail in os.listdir(directory_path):\n",
    "        file_path = directory_path + \"/\" + mail\n",
    "        with open(file_path, \"r\", encoding='latin-1') as m:\n",
    "            mail_dict = parse_message(m)\n",
    "            yield mail_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def parse_message(msg):\n",
    "    body = ''\n",
    "    email = {}\n",
    "    #email['subject'] = ''\n",
    "    in_body = False\n",
    "    exclude_terms = ['URL:', 'Date:', 'Return-Path:']\n",
    "    sw = stopwords.words(\"english\")\n",
    "    \n",
    "    for line in msg:\n",
    "        if line == '\\n':\n",
    "            in_body = True\n",
    "            continue\n",
    "            \n",
    "        if any(term in line for term in exclude_terms):\n",
    "            continue\n",
    "            \n",
    "        \n",
    "        # get rid of html markup\n",
    "        line = re.sub('<[^>]*>', '', line)\n",
    "        \n",
    "        # Sget rid of stopwords\n",
    "        line = ' '.join([word for word in line.split() if word.lower() not in sw])\n",
    "        \n",
    "        \n",
    "        if in_body:\n",
    "            body += line.strip()\n",
    "            email['body'] = body\n",
    "#         elif line.startswith('From:'):\n",
    "#             sender = line.strip()\n",
    "#             sender = sender.replace('\"', '')\n",
    "#             sender = line[5:]\n",
    "#             email['sender'] = sender\n",
    "#         elif line.startswith('Subject:'):\n",
    "#             subject = line.strip()\n",
    "#             subject = line[8:]\n",
    "#             email['subject'] = subject\n",
    "            \n",
    "        # Optionally an else branch could extract more features\n",
    "        \n",
    "    return email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def make_df(path, label):\n",
    "    rows = []\n",
    "    index = []\n",
    "    for i, text in enumerate(convert_files(path)):\n",
    "        rows.append({'body': text['body'], 'label': label}) #'subject': text['subject'], 'sender' : text['sender'],\n",
    "        index.append(i)\n",
    "    \n",
    "    df = pd.DataFrame(rows, index=index)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_ham = make_df('corpus/train-ham', 0)\n",
    "df_spam = make_df('corpus/train-spam', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create Pipeline for e-mail classification\n",
    "    - Vectorization\n",
    "    - Classifier (NaiveBayes / SVM / LogReg)\n",
    "    - (optional spÃ¤ter: feature selection, z.B. IG)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create test and training data\n",
    "df_final = pd.concat([df_ham, df_spam])\n",
    "df_final = df_final.sample(frac=1).reset_index(drop=True)\n",
    "X = df_final.body.values\n",
    "y = df_final.label.values\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#tfidf = TfidfVectorizer(strip_accents=None, lowercase=False, preprocessor=None) #use defaults for smoothing and ngrams\n",
    "\n",
    "# LogisticRegression\n",
    "lr_tfidf = Pipeline([\n",
    "    ('count', CountVectorizer(ngram_range=(1,2))),\n",
    "    ('vect', TfidfTransformer()),\n",
    "    ('clf', LogisticRegression(random_state=0)) # default C to 1.0, mb tune later\n",
    "])\n",
    "\n",
    "# Bayes\n",
    "bayes_tfidf = Pipeline([\n",
    "    ('count', CountVectorizer(ngram_range=(1,2))),\n",
    "    ('vect', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB()) # default smoothing, and settings\n",
    "])\n",
    "\n",
    "# SVM\n",
    "svm_tfidf = Pipeline([\n",
    "    ('count', CountVectorizer(ngram_range=(1,2))),\n",
    "    ('vect', TfidfTransformer()),\n",
    "    ('clf', SVC(C=1.0, gamma=1e-5, kernel='linear')) # default C to 1.0, mb tune later\n",
    "])\n",
    "\n",
    "# RandomForest\n",
    "rf_tfidf = Pipeline([\n",
    "    ('count', CountVectorizer(ngram_range=(1,2))),\n",
    "    ('vect', TfidfTransformer()),\n",
    "    ('clf', RandomForestClassifier()) # default n-trees to 10, mb tune later\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. K-fold Cross-Validation on the data, compare different models\n",
    "    - create k-fold test for each classifier and train data + make predictions on validation-sets\n",
    "    - figure out best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuarcy\n",
      "LR: 0.9630379652420377\n",
      "Bayes: 0.967617662415156\n",
      "SVM: 0.9776982173491462\n",
      "Random Forest: 0.9190432236891176\n",
      "Standard-deviation\n",
      "LR: 0.008921731297032665\n",
      "Bayes: 0.010703246801630402\n",
      "SVM: 0.008444041485558638\n",
      "Random Forest: 0.018160467105540357\n",
      "Variance\n",
      "LR: 7.959728933645215e-05\n",
      "Bayes: 0.00011455949209661145\n",
      "SVM: 7.130183660983532e-05\n",
      "Random Forest: 0.0003298025654914133\n",
      "Confusion - LR\n",
      "[[1758   43]\n",
      " [  78 1395]]\n",
      "Confusion - Bayes\n",
      "[[1739   62]\n",
      " [  44 1429]]\n",
      "Confusion - SVM\n",
      "[[1765   36]\n",
      " [  37 1436]]\n",
      "Confusion - Random Forest\n",
      "[[1620  181]\n",
      " [  84 1389]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "k_fold = KFold(n_splits=10)\n",
    "confusion_lr = np.array([[0,0], [0,0]])\n",
    "confusion_bayes = np.array([[0,0], [0,0]])\n",
    "confusion_svm = np.array([[0,0], [0,0]])\n",
    "confusion_rf = np.array([[0,0], [0,0]])\n",
    "\n",
    "acc_lr = []\n",
    "acc_bayes = []\n",
    "acc_svm = []\n",
    "acc_rf = []\n",
    "\n",
    "for i, (train, validate) in enumerate(k_fold.split(X_train)):\n",
    "    X_tr, X_val = X_train[train], X_train[validate]\n",
    "    y_tr, y_val = y_train[train], y_train[validate]\n",
    "    \n",
    "    lr_tfidf.fit(X_tr, y_tr)\n",
    "    bayes_tfidf.fit(X_tr, y_tr)\n",
    "    svm_tfidf.fit(X_tr, y_tr)\n",
    "    rf_tfidf.fit(X_tr, y_tr)\n",
    "\n",
    "    acc_lr.append(lr_tfidf.score(X_val, y_val))\n",
    "    acc_bayes.append(bayes_tfidf.score(X_val, y_val))\n",
    "    acc_svm.append(svm_tfidf.score(X_val, y_val))\n",
    "    acc_rf.append(rf_tfidf.score(X_val, y_val))\n",
    "    \n",
    "    \n",
    "    \n",
    "    confusion_lr += confusion_matrix(y_val, lr_tfidf.predict(X_val))\n",
    "    confusion_bayes += confusion_matrix(y_val, bayes_tfidf.predict(X_val))\n",
    "    confusion_svm += confusion_matrix(y_val, svm_tfidf.predict(X_val))\n",
    "    confusion_rf += confusion_matrix(y_val, rf_tfidf.predict(X_val))\n",
    "    \n",
    "acc_lr = np.asarray(acc_lr)\n",
    "acc_bayes = np.asarray(acc_bayes)\n",
    "acc_svm = np.asarray(acc_svm)\n",
    "acc_rf = np.asarray(acc_rf)\n",
    "\n",
    "print('Average Accuarcy')\n",
    "print('LR: {}'.format(np.mean(acc_lr)))\n",
    "print('Bayes: {}'.format(np.mean(acc_bayes)))\n",
    "print('SVM: {}'.format(np.mean(acc_svm)))\n",
    "print('Random Forest: {}'.format(np.mean(acc_rf)))\n",
    "\n",
    "print('Standard-deviation')\n",
    "print('LR: {}'.format(np.std(acc_lr)))\n",
    "print('Bayes: {}'.format(np.std(acc_bayes)))\n",
    "print('SVM: {}'.format(np.std(acc_svm)))\n",
    "print('Random Forest: {}'.format(np.std(acc_rf)))\n",
    "\n",
    "print('Variance')\n",
    "print('LR: {}'.format(np.var(acc_lr)))\n",
    "print('Bayes: {}'.format(np.var(acc_bayes)))\n",
    "print('SVM: {}'.format(np.var(acc_svm)))\n",
    "print('Random Forest: {}'.format(np.var(acc_rf)))\n",
    "\n",
    "\n",
    "print('Confusion - LR')\n",
    "print(confusion_lr)\n",
    "print('Confusion - Bayes')\n",
    "print(confusion_bayes)\n",
    "print('Confusion - SVM')\n",
    "print(confusion_svm)\n",
    "print('Confusion - Random Forest')\n",
    "print(confusion_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Tune LR model via GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hagen\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Users\\hagen\\Anaconda3\\lib\\site-packages\\sklearn\\grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: {'clf__C': 100.0, 'clf__penalty': 'l2', 'count__lowercase': False, 'count__ngram_range': (1, 2)}\n",
      "Accuracy (CV): 0.9795357361026268\n",
      "Accuracy (Test): 0.9843304843304843\n"
     ]
    }
   ],
   "source": [
    "# Tune LR \n",
    "from sklearn.grid_search import GridSearchCV\n",
    "param_grid = [\n",
    "    {\n",
    "        'count__ngram_range': [(1,2), (1,3)],\n",
    "        'count__lowercase': [True, False],\n",
    "        'clf__penalty': ['l1', 'l2'],\n",
    "        'clf__C': [0.1, 1.0, 10.0, 100.0]\n",
    "    }\n",
    "]\n",
    "\n",
    "gs_lr_tfidf = GridSearchCV(\n",
    "    lr_tfidf,\n",
    "    param_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "gs_lr_tfidf.fit(X_train, y_train)\n",
    "print('Parameter: {}'.format(gs_lr_tfidf.best_params_))\n",
    "print('Accuracy (CV): {}'.format(gs_lr_tfidf.best_score_))\n",
    "best_classifier = gs_lr_tfidf.best_estimator_\n",
    "print('Accuracy (Test): {}'.format(best_classifier.score(X_test, y_test)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune linear SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: {'clf__C': 10.0, 'count__lowercase': False, 'count__ngram_range': (1, 2)}\n",
      "Accuracy (CV): 0.9807574832009774\n",
      "Accuracy (Test): 0.9807692307692307\n"
     ]
    }
   ],
   "source": [
    "param_grid = [\n",
    "    {\n",
    "        'count__ngram_range': [(1,2), (1,3)],\n",
    "        'count__lowercase': [True, False],\n",
    "        'clf__C': [0.1, 1.0, 10.0, 100.0]\n",
    "    }\n",
    "]\n",
    "\n",
    "gs_svm_tfidf = GridSearchCV(\n",
    "    svm_tfidf,\n",
    "    param_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "gs_svm_tfidf.fit(X_train, y_train)\n",
    "print('Parameter: {}'.format(gs_svm_tfidf.best_params_))\n",
    "print('Accuracy (CV): {}'.format(gs_svm_tfidf.best_score_))\n",
    "best_classifier = gs_svm_tfidf.best_estimator_\n",
    "print('Accuracy (Test): {}'.format(best_classifier.score(X_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = make_df('corpus/test', 0)\n",
    "X = df_test.body.values\n",
    "y = best_classifier.predict(X)\n",
    "my_list = y.tolist()\n",
    "my_list = ['SPAM' if elem == 1  else 'HAM' for elem in my_list]\n",
    "filenames = os.listdir('corpus/test')\n",
    "file = open('result', 'w')\n",
    "\n",
    "for i, item in enumerate(my_list):\n",
    "    file.write(\"{} \\t {}\\n\".format(filenames[i],item))\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result:\n",
    "    The best estimator is a logistic regression in a pipeline with a CountVectorizer & tfidf-transformer\n",
    "    Optimal parameters for Count-Vec: lowercase = False, ngram_range = (1,3)\n",
    "    Optimal parameters for LR: C = 100, penalty = l2\n",
    "    Accuracy on the test-set: 0.979"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Refactor code into train  / predict script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> See uebung3-group7.py for the e-mail classifier script"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
