{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hagen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Read in files\n",
    "    - Loop over ham directory and extract the email body (= Text after the first \\n)\n",
    "    - Save into a pandas data-frame with the label 0\n",
    "    - Same for spam but with label 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "root = \"./\"\n",
    "def convert_files(directory):\n",
    "    # build directory path\n",
    "    directory_path = os.path.join(root, directory)\n",
    "    \n",
    "    for mail in os.listdir(directory_path):\n",
    "        file_path = directory_path + \"/\" + mail\n",
    "        with open(file_path, \"r\", encoding='latin-1') as m:\n",
    "            mail_dict = parse_message(m)\n",
    "            yield mail_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def parse_message(msg):\n",
    "    body = ''\n",
    "    email = {}\n",
    "    email['subject'] = ''\n",
    "    in_body = False\n",
    "    exclude_terms = ['URL:', 'Date:', 'Return-Path:']\n",
    "    sw = stopwords.words(\"english\")\n",
    "    \n",
    "    for line in msg:\n",
    "        if line == '\\n':\n",
    "            in_body = True\n",
    "            continue\n",
    "            \n",
    "        if any(term in line for term in exclude_terms):\n",
    "            continue\n",
    "            \n",
    "        \n",
    "        #get rid of html markup\n",
    "        line = re.sub('<[^>]*>', '', line)\n",
    "        \n",
    "        #get rid of stopwords\n",
    "        line = ' '.join([word for word in line.split() if word.lower() not in sw])\n",
    "        \n",
    "        \n",
    "        if in_body:\n",
    "            body += line.strip()\n",
    "            email['body'] = body\n",
    "        elif line.startswith('From:'):\n",
    "            sender = line.strip()\n",
    "            sender = sender.replace('\"', '')\n",
    "            sender = line[5:]\n",
    "            email['sender'] = sender\n",
    "        elif line.startswith('Subject:'):\n",
    "            subject = line.strip()\n",
    "            subject = line[8:]\n",
    "            email['subject'] = subject\n",
    "            \n",
    "        # Optionally an else branch could extract more features\n",
    "        \n",
    "    return email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def make_df(path, label):\n",
    "    rows = []\n",
    "    index = []\n",
    "    for i, text in enumerate(convert_files(path)):\n",
    "        rows.append({'body': text['body'],'subject': text['subject'], 'sender' : text['sender'], 'label': label}) \n",
    "        index.append(i)\n",
    "    \n",
    "    df = pd.DataFrame(rows, index=index)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_ham = make_df('corpus/train-ham', 0)\n",
    "df_spam = make_df('corpus/train-spam', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create Pipeline for e-mail classification\n",
    "    - Vectorization\n",
    "    - Classifier (NaiveBayes / SVM / LogReg)\n",
    "    - (optional sp√§ter: feature selection, z.B. IG)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create test and training data\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "df_final = pd.concat([df_ham, df_spam])\n",
    "df_final = df_final.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "X = df_final[['body', 'subject', 'sender']]\n",
    "y = df_final.label.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Create ItemSelector class to be used in FeatureUnion later. (To handle multiple text-columns in X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'transformer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-69-de9854ad6c99>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[1;34m'body'\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[1;34m'subject'\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m             \u001b[1;34m'sender'\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m         },\n\u001b[0;32m     34\u001b[0m     )),\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'transformer'"
     ]
    }
   ],
   "source": [
    "# create Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_tfidf = Pipeline([\n",
    "    # Create a feature union to combine the columns\n",
    "    ('union', FeatureUnion(\n",
    "        transformer = [\n",
    "            # body pipeline\n",
    "            ('body', Pipeline([\n",
    "                ('selector', ItemSelector(key='body')),\n",
    "                ('tfidf', TfidfVectorizer()),\n",
    "            ])),\n",
    "            \n",
    "            # subject pipeline\n",
    "            ('subject', Pipeline([\n",
    "                ('selector', ItemSelector(key='subject')),\n",
    "                ('tfidf', TfidfVectorizer()),\n",
    "            ])),\n",
    "            \n",
    "            # sender pipeline\n",
    "            ('sender', Pipeline([\n",
    "                ('selector', ItemSelector(key='sender')),\n",
    "                ('tfidf', TfidfVectorizer()),\n",
    "            ])),\n",
    "        ],\n",
    "        transformer_weights={\n",
    "            'body' : 1.0,\n",
    "            'subject' : 1.0,\n",
    "            'sender' : 1.0,\n",
    "        },\n",
    "    )),\n",
    "    ('clf', LogisticRegression(random_state=0)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,3), lowercase=False)\n",
    "X = sp.hstack(X.apply(lambda col: vectorizer.fit_transform(col)))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#tfidf = TfidfVectorizer(strip_accents=None, lowercase=False, preprocessor=None) #use defaults for smoothing and ngrams\n",
    "\n",
    "# LogisticRegression\n",
    "lr_tfidf = Pipeline([\n",
    "    ('count', CountVectorizer(ngram_range=(1,2))),\n",
    "    ('vect', TfidfTransformer()),\n",
    "    ('clf', LogisticRegression(random_state=0)) # default C to 1.0, mb tune later\n",
    "])\n",
    "\n",
    "# Bayes\n",
    "bayes_tfidf = Pipeline([\n",
    "    ('count', CountVectorizer(ngram_range=(1,2))),\n",
    "    ('vect', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB()) # default smoothing, and settings\n",
    "])\n",
    "\n",
    "# SVM\n",
    "svm_tfidf = Pipeline([\n",
    "    ('count', CountVectorizer(ngram_range=(1,2))),\n",
    "    ('vect', TfidfTransformer()),\n",
    "    ('clf', SVC(C=1.0, gamma=1e-5)) # default C to 1.0, mb tune later\n",
    "])\n",
    "\n",
    "# RandomForest\n",
    "rf_tfidf = Pipeline([\n",
    "    ('count', CountVectorizer(ngram_range=(1,2))),\n",
    "    ('vect', TfidfTransformer()),\n",
    "    ('clf', RandomForestClassifier()) # default n-trees to 10, mb tune later\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. K-fold Cross-Validation on the data, compare different models\n",
    "    - create k-fold test for each classifier and train data + make predictions on validation-sets\n",
    "    - figure out best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9801526717557252 (Fold: 0)\n",
      "Test Accuracy: 0.983206106870229 (Fold: 0)\n",
      "Test Accuracy: 0.5221374045801527 (Fold: 0)\n",
      "Test Accuracy: 0.9358778625954198 (Fold: 0)\n",
      "Test Accuracy: 0.9770992366412213 (Fold: 1)\n",
      "Test Accuracy: 0.9893129770992366 (Fold: 1)\n",
      "Test Accuracy: 0.5450381679389313 (Fold: 1)\n",
      "Test Accuracy: 0.934351145038168 (Fold: 1)\n",
      "Test Accuracy: 0.9770992366412213 (Fold: 2)\n",
      "Test Accuracy: 0.9786259541984733 (Fold: 2)\n",
      "Test Accuracy: 0.5312977099236641 (Fold: 2)\n",
      "Test Accuracy: 0.9282442748091603 (Fold: 2)\n",
      "Test Accuracy: 0.9679389312977099 (Fold: 3)\n",
      "Test Accuracy: 0.9740458015267176 (Fold: 3)\n",
      "Test Accuracy: 0.5587786259541985 (Fold: 3)\n",
      "Test Accuracy: 0.9297709923664123 (Fold: 3)\n",
      "Test Accuracy: 0.9770642201834863 (Fold: 4)\n",
      "Test Accuracy: 0.9709480122324159 (Fold: 4)\n",
      "Test Accuracy: 0.5076452599388379 (Fold: 4)\n",
      "Test Accuracy: 0.9617737003058104 (Fold: 4)\n",
      "Confusion - LR\n",
      "[[1673   72]\n",
      " [   7 1522]]\n",
      "Confusion - Bayes\n",
      "[[1712   33]\n",
      " [  35 1494]]\n",
      "Confusion - SVM\n",
      "[[1745    0]\n",
      " [1529    0]]\n",
      "Confusion - Random Forest\n",
      "[[1679   66]\n",
      " [ 137 1392]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "k_fold = KFold(n_splits=5)\n",
    "confusion_lr = np.array([[0,0], [0,0]])\n",
    "confusion_bayes = np.array([[0,0], [0,0]])\n",
    "confusion_svm = np.array([[0,0], [0,0]])\n",
    "confusion_rf = np.array([[0,0], [0,0]])\n",
    "\n",
    "lr = LogisticRegression(random_state = 0)\n",
    "bayes = MultinomialNB()\n",
    "svm = SVC(C=1.0, gamma=1e-5)\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "for i, (train, validate) in enumerate(k_fold.split(X_train)):\n",
    "    X_tr, X_val = X_train[train], X_train[validate]\n",
    "    y_tr, y_val = y_train[train], y_train[validate]\n",
    "    \n",
    "    lr.fit(X_tr, y_tr)\n",
    "    bayes.fit(X_tr, y_tr)\n",
    "    svm.fit(X_tr, y_tr)\n",
    "    rf.fit(X_tr, y_tr)\n",
    "\n",
    "    print(\"Test Accuracy: {} (Fold: {})\".format(lr.score(X_val, y_val), i))\n",
    "    print(\"Test Accuracy: {} (Fold: {})\".format(bayes.score(X_val, y_val), i))\n",
    "    print(\"Test Accuracy: {} (Fold: {})\".format(svm.score(X_val, y_val), i))\n",
    "    print(\"Test Accuracy: {} (Fold: {})\".format(rf.score(X_val, y_val), i))\n",
    "    \n",
    "    confusion_lr += confusion_matrix(y_val, lr.predict(X_val))\n",
    "    confusion_bayes += confusion_matrix(y_val, bayes.predict(X_val))\n",
    "    confusion_svm += confusion_matrix(y_val, svm.predict(X_val))\n",
    "    confusion_rf += confusion_matrix(y_val, rf.predict(X_val))\n",
    "    \n",
    "    \n",
    "print('Confusion - LR')\n",
    "print(confusion_lr)\n",
    "print('Confusion - Bayes')\n",
    "print(confusion_bayes)\n",
    "print('Confusion - SVM')\n",
    "print(confusion_svm)\n",
    "print('Confusion - Random Forest')\n",
    "print(confusion_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# svm looks odd, kinda as if it just classifies naive --> need to check later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Tune LR model via GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hagen\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Users\\hagen\\Anaconda3\\lib\\site-packages\\sklearn\\grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: {'C': 100.0, 'penalty': 'l2'}\n",
      "Accuracy (CV): 0.9792302993280391\n",
      "Accuracy (Test): 0.9821937321937322\n"
     ]
    }
   ],
   "source": [
    "# Tune LR \n",
    "from sklearn.grid_search import GridSearchCV\n",
    "param_grid = [\n",
    "    {\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'C': [0.1, 1.0, 10.0, 100.0]\n",
    "    }\n",
    "]\n",
    "\n",
    "gs_lr = GridSearchCV(\n",
    "    lr,\n",
    "    param_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "gs_lr.fit(X_train, y_train)\n",
    "print('Parameter: {}'.format(gs_lr.best_params_))\n",
    "print('Accuracy (CV): {}'.format(gs_lr.best_score_))\n",
    "best_classifier = gs_lr.best_estimator_\n",
    "print('Accuracy (Test): {}'.format(best_classifier.score(X_test, y_test)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = make_df('corpus/test', 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = df_test[['body', 'subject', 'sender']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = df_test[['body', 'subject', 'sender']]\n",
    "X = sp.hstack(X.apply(lambda col: vectorizer.transform(col)))\n",
    "clf = best_classifier\n",
    "y = clf.predict(X)\n",
    "with open(result_file, 'w') as r:\n",
    "        r.write(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=0, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_classifier.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result:\n",
    "    The best estimator is a logistic regression in a pipeline with a CountVectorizer \n",
    "    Optimal parameters for Count-Vec: lowercase = False, ngram_range = (1,3)\n",
    "    Optimal parameters for LR: C = 100, penalty = l2\n",
    "    Accuracy on the test-set: 0.982"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Refactor code into train  / predict script"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
