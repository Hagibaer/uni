{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%run \"KNN_own_implementation.ipynb\" \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numdifftools as nd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Train and Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data', sep=',', header=None)\n",
    "df_test = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test', sep=',', header=None, skiprows=1)\n",
    "df_train.columns = [\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education-num\", \"martial-status\", \"occupation\", \"relationship\",\n",
    "             \"race\", \"sex\", \"capital-gain\", \"capital-loss\", \"hours-per-week\", \"native-country\", \"class\"]\n",
    "df_test.columns = [\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education-num\", \"martial-status\", \"occupation\", \"relationship\",\n",
    "             \"race\", \"sex\", \"capital-gain\", \"capital-loss\", \"hours-per-week\", \"native-country\", \"class\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert workclass into  categories: \n",
    "1. Private\n",
    "2. self-employed = { “Self-empnot-inc”,“Self-emp-inc”}\n",
    "3. government-employed = {“Federal-gov”, “Local-gov”, “Stategov”}\n",
    "4. unemployed = {“Without-pay”, “Never-worked”}\n",
    "\n",
    "Convert education into these categories:\n",
    "1. higher-education = {“Doctorate”, “Masters”, “Bachelors”}\n",
    "2. high-school = {“HS-grad”, “Somecollege”, “Prof-school”, “Assoc-acdm”, “Assoc-voc”}\n",
    "3. lower-education = {all the Rest}\n",
    "\n",
    "Add column 'sex'\n",
    "\n",
    "Add all continouus columns.\n",
    "\n",
    "Drop all the Rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def categorize_workclass(x):\n",
    "    self_employed = [\"Self-emp-not-inc\", \"Self-emp-inc\"]\n",
    "    government_employed = [\"Federal-gov\", \"Local-gov\", \"State-gov\"]\n",
    "    unemployed = [\"Without-pay\", \"Never-worked\"]\n",
    "    x = x.strip()\n",
    "    \n",
    "    if x == 'Private':\n",
    "        next\n",
    "    \n",
    "    if x in self_employed:\n",
    "        x = 'self-employed'\n",
    "        \n",
    "    if x in government_employed:\n",
    "        x = 'government-employed'\n",
    "        \n",
    "    if x in unemployed:\n",
    "        x = 'unemployed'\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def categorize_education(x):\n",
    "    higher_education = [\"Doctorate\", \"Masters\", \"Bachelors\"]\n",
    "    high_school = [\"HS-grad\", \"Some-college\", \"Prof-school\", \"Assoc-acdm\", \"Assoc-voc\"]\n",
    "    x = x.strip()\n",
    "    \n",
    "    if x in higher_education:\n",
    "        x = 'higher_education'\n",
    "    elif x in high_school:\n",
    "        x = 'high-school'    \n",
    "    else:\n",
    "        x = 'lower_education'\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode_class(x):\n",
    "    x = x.strip()\n",
    "    if x == '<=50K' or x == '<=50K.':\n",
    "        x = 0\n",
    "    else:\n",
    "        x = 1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['workclass'] =  df_train['workclass'].apply(lambda x: categorize_workclass(x))\n",
    "df_test['workclass'] =  df_test['workclass'].apply(lambda x: categorize_workclass(x))\n",
    "df_train['education'] =  df_train['education'].apply(lambda x: categorize_education(x))\n",
    "df_test['education'] =  df_test['education'].apply(lambda x: categorize_education(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[df_train.workclass != '?']\n",
    "df_test = df_test[df_test.workclass != '?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = df_train.dropna()\n",
    "df_test = df_test.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Select relevant categories\n",
    "y_train = df_train['class']\n",
    "y_test = df_test['class']\n",
    "\n",
    "df_train = df_train[['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week', 'workclass', 'education', 'sex']]\n",
    "df_test = df_test[['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week', 'workclass', 'education', 'sex']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode categorical input with dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make use of pd.get_dummies --> it encodes every categorical value via one-hot-encoding. \n",
    "# This results in 15 instead of 12 columns.\n",
    "df_train = pd.get_dummies(df_train)\n",
    "df_test = pd.get_dummies(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = y_train.apply(lambda x: encode_class(x))\n",
    "y_test = y_test.apply(lambda x: encode_class(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_space = df_train.values\n",
    "X_test = df_test.values\n",
    "input_labels = y_train.values\n",
    "test_labels = y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# standardize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "sc.fit(input_space)\n",
    "input_space = sc.transform(input_space)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test with knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " # knn(input_space=input_space, input_labels=input_labels, X_test=X_test, k=3) --> that is some lazy boy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sigmoid function to map results between 0-1\n",
    "def sigmoid(result):\n",
    "    result = np.exp(result) / (1 + np.exp(result))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_log_likelihood(y_target, beta, input_space):\n",
    "    # Formel: Sum(yi * Beta * xi - ln(1 + exp(Beta*xi)))\n",
    "    # TODO: 1-padding? --> No is already in get_coefficients()\n",
    "    ll = np.sum(y_target * np.dot(input_space, beta) - np.log(1 + np.exp(np.dot(input_space, beta))))\n",
    "    return ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hessian(input_space, beta): \n",
    "    # since the hessian matrix for H(Beta) is given as 2nd par. deriv. from l(beta)/dBeta dBeta^T\n",
    "    # Formula (after deriving): Sum(- (xi^2*e^BetaT*xi) / (1 + e^(BetaT*xi))^2  --> not sure, and since it doesnt work its probably wrong\n",
    "    nomiator = np.dot(input_space**2, np.exp(np.dot(input_space, beta))) * -1\n",
    "    denomiator = (1 + np.dot(input_space, beta))**2\n",
    "    hessian = np.divide(nomiator, denomiator)\n",
    "    return hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_coefficients(input_space, y_target, steps, gamma,  beta_init_multiplier=0, method='gradient'):\n",
    "    # pad input space with 1\n",
    "    input_space = np.hstack((np.ones((input_space.shape[0],1)), input_space))\n",
    "    \n",
    "    # initialize beta-vector with values\n",
    "    beta = np.ones(input_space.shape[1])*beta_init_multiplier\n",
    "    \n",
    "    for i in range(steps):\n",
    "        continouus_prediction = np.dot(input_space, beta) # weil Y = XB (B = Beta)\n",
    "        scaled_prediction = sigmoid(continouus_prediction)\n",
    "        \n",
    "        error = y_target - scaled_prediction\n",
    "        \n",
    "        gradient =  np.dot(input_space.T, error)\n",
    "        \n",
    "        if method == 'gradient':\n",
    "            beta = beta + gamma * gradient\n",
    "        elif method == 'newton-raphson':\n",
    "            # Implement update rule here -- still not working!\n",
    "            hess = hessian(input_space, beta)\n",
    "            hessian_inverse = np.linalg.inv(hess)\n",
    "            beta = beta - np.dot(hessian_inverse, gradient)\n",
    "        else:\n",
    "            raise('Please specify a correct update rule for beta')\n",
    "        \n",
    "    # print('Log-Likelihood: {}'.format(get_log_likelihood(y_target, beta, input_space)))\n",
    "    \n",
    "    return beta\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = get_coefficients(input_space, y_target=input_labels,beta_init_multiplier=0 ,steps = 50000, gamma=5e-5, method='gradient')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.37181441  0.58388794  0.04567079  0.82235614  2.38944114  0.27830111\n",
      "  0.3953425   0.01596662  0.02938879 -0.03254329 -0.25725924  0.0078903\n",
      "  0.0181743  -0.03485601 -0.28269167  0.28269167]\n"
     ]
    }
   ],
   "source": [
    "print(beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check result against sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.36981997] [[ 0.58388985  0.04567042  0.82236197  2.38952113  0.27830123  0.39534463\n",
      "   0.01389009  0.02774283 -0.03407509 -0.18141268  0.00788978  0.0181702\n",
      "  -0.03484997 -0.28269125  0.28269125]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(fit_intercept=True, C = 1e15) # C value =  get rid of L2-regularization\n",
    "clf.fit(input_space, input_labels)\n",
    "print(clf.intercept_, clf.coef_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predict test-data with different beta-initializations and compute average accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = np.hstack((np.ones((X_test.shape[0],1)), X_test)) # pad test-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy: 0.8177307742525135\n"
     ]
    }
   ],
   "source": [
    "accuracies = []\n",
    "\n",
    "for i in np.arange(0,1, 0.1):\n",
    "    beta = get_coefficients(input_space, input_labels, steps=50000, gamma=5e-5, beta_init_multiplier=i)\n",
    "    \n",
    "    y_predict = np.dot(X_test, beta)\n",
    "    y_predict = np.round(sigmoid(y_predict))\n",
    "    num_correct = len(test_labels) - sum(abs(test_labels - y_predict)) # total number of labels - wrong classified\n",
    "    accuracy = num_correct / len(test_labels)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "print(\"Average accuracy: {}\".format(sum(accuracies)/float(len(accuracies))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO:  Newton-Raphson-opt, korrigieren nach lecture"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
