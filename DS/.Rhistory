# Variables and Classes
a <- 3.0
b <- 4.5
class(a)
class(b) == 'character'
a^2 + 1/b
sqrt(a*b)
# Matrix algebra
A = matrix(c(1,4,7,2,5,8,3,6,10), nrow=3)
B = matrix(c(1:9), nrow=3)
y = matrix(c(1:3))
a * A
A %*% B
invA = solve(A)
A %*%invA
t(B)
B[1,] = c(1,1,1)
ols = function(A, y) solve(t(A)%*%A)%*%t(A)%*%y
ols(A, y)
# Indexing
# A B y (anzeigen)
A[3,2] * B[2,1]
A[1,] * B[,3]
y[y>1]
A[,2][A[,1] >= 4]
# Custom function
standardize = function(x) {
if (is.numeric(x)){
mu = mean(x)
std = sd(x)
return((x - mu) / std)
}
else{
return(x)
}
}
a = c(-100, -25, -10, 0, 10, 25, 100)
standardize(a)
# Using inbuilt-functions
dnorm(x = c(1,2,3,6))
x = seq(-2,2, by=0.2)
nvValues = dnorm(x, mean = 0, sd = 1)
plot(x = x, y = nvValues, type='l')
B <- matrix(c(1,1,0,0,1,1,0,0,0,0,1,1,0,0,1,1), ncol=4)
B
eigen(B)
A <- matrix(c(1, sqrt(2), sqrt(2), 0),ncol = 2)
eigen(A)
# Load prepared data
X_train = read.csv(file="Submission_1/data/X_train.csv", header=TRUE, sep=",")
X_test = read.csv(file="Submission_1/data/X_test.csv", header=TRUE, sep=",")
y_train = read.csv(file="Submission_1/data/y_train.csv", header=FALSE, sep=",")
# setup
setwd("C:/Users/Hagen/uni/DS")
# Load prepared data
X_train = read.csv(file="Submission_1/data/X_train.csv", header=TRUE, sep=",")
X_test = read.csv(file="Submission_1/data/X_test.csv", header=TRUE, sep=",")
y_train = read.csv(file="Submission_1/data/y_train.csv", header=FALSE, sep=",")
X_train
sapply(X_train, is.numeric)
# Factorize columns
X_train$item_size_normal <- as.factor(X_train$item_size_normal)
for (col in colnames(X_train)){
if (!startsWith(col, 'X')){
X_train[col] <- as.factor(X_train[col])
}
}
X_train = read.csv(file="Submission_1/data/X_train.csv", header=TRUE, sep=",")
X_test = read.csv(file="Submission_1/data/X_test.csv", header=TRUE, sep=",")
y_train = read.csv(file="Submission_1/data/y_train.csv", header=FALSE, sep=",")
for (col in colnames(X_train)){
if (!startsWith(col, 'X')){
X_train[col] <- as.factor(X_train[col])
}
}
colnames(X_train)
sprintf(col)
# Factorize columns
#X_train$item_size_normal <- as.factor(X_train$item_size_normal)
for(col in colnames(X_train)){
sprintf(col)
}
lnames(X_train)){
sprintf(i)
}
for(i in colnames(X_train)){
sprintf(i)
}
colnames(X_train)){
print(i)
}
for(i in colnames(X_train)){
print(i)
}
X_train['item_size_normal']
startsWith('item_size_normal', 'X')
for(i in colnames(X_train)){
print(i)
}
startsWith('X4', 'X')
!startsWith('X4', 'X')
for(i in colnames(X_train)){
if(!startsWith(i, 'X')){
print(i)
}
}
X_train['order_month_1'] <- as.factor(X_train['order_month_1'])
y_train = read.csv(file="Submission_1/data/y_train.csv", header=FALSE, sep=",")
# setup
setwd("C:/Users/Hagen/uni/DS")
# Load prepared data
X_train = read.csv(file="Submission_1/data/X_train.csv", header=TRUE, sep=",")
X_test = read.csv(file="Submission_1/data/X_test.csv", header=TRUE, sep=",")
y_train = read.csv(file="Submission_1/data/y_train.csv", header=FALSE, sep=",")
# Best model from the set (Logistic_Regression, Decision_Tree, Random_Forest, Bayes) was LR (tested in Python skript)
X_train <- data.matrix(X_train)
X_test <- data.matrix(X_test)
y_train <- as.factor(y_train$V1)
# Train model
library('glmnet')
rlm <-glmnet(x=X_train, y=y_train, family='binomial', standardize=TRUE, alpha=1)
#predict
rlm_predict <- predict(rlm, newx=X_test ,type = "response", s=0.01)
rlm_predict <- ifelse(rlm_predict > 0.5, 1, 0)
# Create a dataframe with two columns: order_item_id = 100001 - 150000 and return = prediction
order_item_id = seq(from=100001, to=150000, by = 1)
return = rlm_predict
return <- sapply(return, function(x) paste0("'", x)) # to outsmart excel
df = data.frame(order_item_id, return)
colnames(df) <- c('order_item_id', 'return')
write.csv(df, '589187_mohr.csv', row.names = FALSE)
