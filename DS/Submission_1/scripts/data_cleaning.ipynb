{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read raw data\n",
    "df_train = pd.read_csv('../data/BADS_WS1718_known.csv', sep=',', na_values=['?', 'not_reported'])\n",
    "df_test = pd.read_csv('../data/BADS_WS1718_class.csv', sep=',', na_values=['?', 'not_reported'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# just to prove pythons superiority compared to R\n",
    "#for col in df_train.columns:\n",
    "#    print(df_train.groupby(col)['order_item_id'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#for col in df_test.columns:\n",
    "#    print(df_test.groupby(col)['order_item_id'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1. Drop order_item_id -> no value\n",
    "df_train = df_train.drop(['order_item_id'], axis=1)\n",
    "df_test = df_test.drop(['order_item_id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2. Convert date columns to datetime format:  order_date, delivery-date, user_reg_date, user_dob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "date_columns = ['order_date', 'delivery_date', 'user_reg_date', 'user_dob']\n",
    "for date in date_columns:\n",
    "    df_train[date] = pd.to_datetime(df_train[date], format=\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for date in date_columns:\n",
    "    df_test[date] = pd.to_datetime(df_test[date], format=\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3. Calculate additional columns\n",
    "# 3.1 Length of customership  (LOC) (order_date - user_reg_date )\n",
    "df_train['LOC'] = (df_train.order_date - df_train.user_reg_date) / np.timedelta64(1, 'D')\n",
    "df_test['LOC'] = (df_test.order_date - df_test.user_reg_date) / np.timedelta64(1, 'D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#3.2 age of the customers df['age'] at the time of order (order_date - user_dob)\n",
    "df_train['age'] = np.round((df_train.order_date - df_train.user_dob) / np.timedelta64(1, 'Y'))\n",
    "df_test ['age'] = np.round((df_test.order_date - df_test.user_dob) / np.timedelta64(1, 'Y'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#3.3 delivery-time (DT) (order_date - delivery_date)\n",
    "df_train['DT'] = (df_train.delivery_date - df_train.order_date) / np.timedelta64(1, 'D')\n",
    "df_test['DT'] = (df_test.delivery_date - df_test.order_date) / np.timedelta64(1, 'D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#3.4 Number of past purchases (NPP)\n",
    "past_purchases = df_train.user_id.value_counts()\n",
    "df_train['npp'] = 0\n",
    "npp = []\n",
    "for elem in zip(df_train.user_id, df_train.npp):\n",
    "    npp.append(past_purchases[elem[0]])\n",
    "df_train.npp = npp\n",
    "\n",
    "past_purchases = df_test.user_id.value_counts()\n",
    "df_test['npp'] = 0\n",
    "npp = []\n",
    "for elem in zip(df_test.user_id, df_test.npp):\n",
    "    npp.append(past_purchases[elem[0]])\n",
    "df_test.npp = npp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 3.5 Return rate of the customer\n",
    "# Update: NEVER use the target variable in feature engineering because it messes up the whole model\n",
    "# df_train.groupby('user_id')['return'].sum() --> I don't include it because it would not be independent from the npp column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3.6 split order_date, delivery_date into --> *_month, *year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train['order_month'], df_train['order_year'] = df_train.order_date.dt.month, df_train.order_date.dt.year \n",
    "df_test['order_month'], df_test['order_year'] = df_test.order_date.dt.month, df_test.order_date.dt.year\n",
    "df_train['delivery_month'], df_train['delivery_year'] = df_train.delivery_date.dt.month, df_train.delivery_date.dt.year\n",
    "df_test['delivery_month'], df_test['delivery_year'] = df_test.delivery_date.dt.month, df_train.delivery_date.dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['order_date', 'delivery_date', 'item_id', 'item_size', 'item_color',\n",
       "       'brand_id', 'item_price', 'user_id', 'user_title', 'user_dob',\n",
       "       'user_state', 'user_reg_date', 'return', 'LOC', 'age', 'DT', 'npp',\n",
       "       'order_month', 'order_year', 'delivery_month', 'delivery_year'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop now obsolete columns\n",
    "df_train = df_train.drop(['order_date', 'delivery_date', 'user_dob', 'user_reg_date'], axis=1)\n",
    "df_test =  df_test.drop(['order_date', 'delivery_date', 'user_dob', 'user_reg_date'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # useful helper\n",
    "# with pd.option_context('display.max_rows', None, 'display.max_columns', 3):\n",
    "#     print(#whatever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 5. Strategy for columns with a lot of different class labels\n",
    "# 5.1 item_id --> 3 Categories: popular item (bought >= 300 times), normal item (300 > x > 50), rare item (50 >= num_bought)\n",
    "def categorize_item_id(x, item_id_counts):\n",
    "    x = item_id_counts[x]\n",
    "    \n",
    "    if x > 300:\n",
    "        x = 'popular'\n",
    "    elif x > 50:\n",
    "        x = 'normal'\n",
    "    else:\n",
    "        x = 'rare'\n",
    "        \n",
    "    return x\n",
    "\n",
    "item_id_train_counts = df_train.item_id.value_counts()\n",
    "item_id_test_counts = df_test.item_id.value_counts()\n",
    "\n",
    "df_train['item_popularity'] = df_train.item_id.apply(lambda x: categorize_item_id(x, item_id_train_counts))\n",
    "df_test['item_popularity'] = df_test.item_id.apply(lambda x: categorize_item_id(x, item_id_test_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 5.2 item_size --> join big letters and small letters into same group if the same (L + l); popular normal rare\n",
    "def categorize_item_size(x, item_size_count):\n",
    "        \n",
    "    count = item_size_count[x]\n",
    "    if count > 2000:\n",
    "        x = 'popular'\n",
    "    elif count > 500:\n",
    "        x = 'normal'\n",
    "    else:\n",
    "        x = 'rare'\n",
    "    return x\n",
    "\n",
    "df_train.item_size = df_train.item_size.apply(lambda x: x.lower())\n",
    "df_test.item_size = df_test.item_size.apply(lambda x: x.lower())\n",
    "\n",
    "item_size_train_counts = df_train.item_size.value_counts()\n",
    "item_size_test_counts = df_test.item_size.value_counts()\n",
    "\n",
    "df_train.item_size = df_train.item_size.apply(lambda x: categorize_item_size(x, item_size_train_counts))\n",
    "df_test.item_size = df_test.item_size.apply(lambda x: categorize_item_size(x, item_size_test_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "black             17896\n",
      "blue              10141\n",
      "grey               8703\n",
      "red                8158\n",
      "brown              6849\n",
      "green              6662\n",
      "white              4027\n",
      "purple             3723\n",
      "petrol             3338\n",
      "ocher              2404\n",
      "olive              2335\n",
      "denim              2113\n",
      "berry              2030\n",
      "anthracite         2023\n",
      "stained            1952\n",
      "mocca              1798\n",
      "pink               1377\n",
      "aquamarine         1207\n",
      "aubergine          1052\n",
      "ash                 989\n",
      "orange              829\n",
      "ecru                822\n",
      "turquoise           755\n",
      "beige               657\n",
      "dark denim          652\n",
      "nature              637\n",
      "magenta             627\n",
      "iron                459\n",
      "terracotta          457\n",
      "bordeaux            436\n",
      "azure               415\n",
      "navy                369\n",
      "khaki               369\n",
      "yellow              323\n",
      "coral               313\n",
      "basalt              310\n",
      "pallid              309\n",
      "blau                283\n",
      "curry               222\n",
      "graphite            186\n",
      "hibiscus            144\n",
      "cognac              142\n",
      "mint                134\n",
      "mahagoni            125\n",
      "ancient             115\n",
      "aqua                104\n",
      "fuchsia              99\n",
      "floral               94\n",
      "striped              84\n",
      "silver               71\n",
      "ingwer               69\n",
      "darkblue             67\n",
      "ivory                54\n",
      "mango                50\n",
      "champagner           46\n",
      "apricot              41\n",
      "jade                 39\n",
      "dark navy            38\n",
      "dark oliv            31\n",
      "habana               31\n",
      "baltic blue          29\n",
      "gold                 28\n",
      "cobalt blue          21\n",
      "dark garnet          16\n",
      "kanel                12\n",
      "almond               10\n",
      "brwon                 7\n",
      "dark grey             7\n",
      "creme                 6\n",
      "antique pink          6\n",
      "caramel               5\n",
      "curled                5\n",
      "currant purple        5\n",
      "copper coin           4\n",
      "aviator               4\n",
      "ebony                 4\n",
      "crimson               2\n",
      "vanille               2\n",
      "lemon                 2\n",
      "amethyst              2\n",
      "avocado               1\n",
      "oliv                  1\n",
      "opal                  1\n",
      "perlmutt              1\n",
      "Name: item_color, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', 3):\n",
    "    print(df_train.item_color.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 5.3 item_color --> popular normal rare\n",
    "def categorize_item_color(x, item_color_count):\n",
    "    \n",
    "    count = item_color_count[x]\n",
    "    if count > 4000:\n",
    "        x = 'popular'\n",
    "    elif count > 300:\n",
    "        x = 'normal'\n",
    "    else:\n",
    "        x = 'rare'\n",
    "\n",
    "    return x\n",
    "\n",
    "# Convert NaNs to other\n",
    "df_train.item_color = df_train.item_color.fillna('other')\n",
    "df_test.item_color = df_test.item_color.fillna('other')\n",
    "\n",
    "item_color_train_counts = df_train.item_color.value_counts()\n",
    "item_color_test_counts = df_test.item_color.value_counts()\n",
    "\n",
    "df_train.item_color = df_train.item_color.apply(lambda x: categorize_item_color(x, item_color_train_counts))\n",
    "df_test.item_color = df_test.item_color.apply(lambda x: categorize_item_color(x, item_color_test_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 5.4 brand_id --> if count > 2000 : popular; 2000 >= count > 500 : normal; Rest --> rare\n",
    "def categorize_brand_id(x, brand_id_count):\n",
    "    \n",
    "    count = brand_id_count[x]\n",
    "    \n",
    "    if count > 2000:\n",
    "        x = 'popular'\n",
    "    elif count > 500:\n",
    "        x = 'normal'\n",
    "    else:\n",
    "        x = 'rare'\n",
    "        \n",
    "    return x\n",
    "\n",
    "brand_id_train_counts = df_train.brand_id.value_counts()\n",
    "brand_id_test_counts = df_test.brand_id.value_counts()\n",
    "\n",
    "df_train['brand_popularity'] = df_train.brand_id.apply(lambda x: categorize_brand_id(x, brand_id_train_counts))\n",
    "df_test['brand_popularity'] = df_test.brand_id.apply(lambda x: categorize_brand_id(x, brand_id_test_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 5.5 delivery_month --> make nas to 'other'\n",
    "df_train.delivery_month = df_train.delivery_month.fillna('other')\n",
    "df_test.delivery_month = df_test.delivery_month.fillna('other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 5.6 delivery_year --> make nas to other\n",
    "df_train.delivery_year = df_train.delivery_year.fillna('other')\n",
    "df_test.delivery_year = df_test.delivery_year.fillna('other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 5.6 user_id --> nothing really that one could do here, since we already got number of purchases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Again drop now unnecessary columns\n",
    "df_train = df_train.drop(['item_id', 'brand_id', 'user_id'], axis=1)\n",
    "df_test =  df_test.drop(['item_id', 'brand_id', 'user_id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### All columns are fine now in terms of data type and structure ###\n",
    "\n",
    "### Check columns for plausability ###\n",
    "\n",
    "# item_size                      | Train: Okay | Test: Okay\n",
    "# item_color                     | Train: Okay | Test: Okay\n",
    "# item_price                     --> Zero values, and two 999 values. Cant for sure be outliers --> stay; Rest okay\n",
    "# user_title                     | Train: Okay | Test: Okay\n",
    "# user_state                     | Train: Okay | Test: Okay\n",
    "# LOC                            | Train: Okay | Test: Okay\n",
    "# age                            | Train: needs work | Test: needs work --> Some are older than 100 and  smaller 15 !! --FIXED--\n",
    "# DT                             | Train: work | Test: work --> bunch of negative delivery times   --FIXED--\n",
    "# npp                            | Train: Okay | Test: Okay \n",
    "# order_month                    | Train: Okay | Test: Okay\n",
    "# order_year                     | Train: Okay | Test: Okay\n",
    "# delivery_month                 | Train: Okay | Test: Okay --> NaNs bei beiden --FIXED--\n",
    "# delivery_year                  | Train: Okay | Test: Okay --> NaNs bei beiden  und 1990 (unsinnig) --FIXED--\n",
    "# item_popularity                | Train: Okay | Test: Okay\n",
    "# brand_popularity               | Train: Okay | Test: Okay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1. fix age column: Every age > 80 == 80; and every age < 18 == 18\n",
    "def fix_age(x):\n",
    "    if x > 80:\n",
    "        x = 80\n",
    "    elif x < 18:\n",
    "        x = 18\n",
    "    return x\n",
    "\n",
    "# Fill NAs with median\n",
    "median_train_age = df_train.age.median()\n",
    "median_test_age = df_test.age.median()\n",
    "\n",
    "df_train.age = df_train.age.fillna(median_train_age)\n",
    "df_test.age = df_test.age.fillna(median_test_age)\n",
    "\n",
    "df_train.age = df_train.age.apply(lambda x: fix_age(x))\n",
    "df_test.age = df_test.age.apply(lambda x: fix_age(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2.fix DT column: negative values come from 1990 delivery time --> impute to median delivery_time (mean from positive values)\n",
    "def fix_DT(x, mean_DT):\n",
    "    if x < 0:\n",
    "        x = mean_DT\n",
    "    return x\n",
    "\n",
    "median_train_DT = df_train.DT[df_train.DT >= 0].median()\n",
    "median_test_DT = df_test.DT[df_test.DT >= 0].median()\n",
    "\n",
    "#Fill NA DTs with median\n",
    "df_train.DT = df_train.DT.fillna(median_train_DT)\n",
    "df_test.DT = df_test.DT.fillna(median_test_DT)\n",
    "\n",
    "df_train.DT = df_train.DT.apply(lambda x: fix_DT(x, median_train_DT))\n",
    "df_test.DT = df_test.DT.apply(lambda x: fix_DT(x, median_test_DT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3. fix delivery_year --> 1990 values will just be smoothed to 2012\n",
    "def fix_delivery_year(x):\n",
    "    if x == 'other':\n",
    "        return x\n",
    "    if x < 2012:\n",
    "        x = 2012\n",
    "    return x\n",
    "\n",
    "df_train.delivery_year = df_train.delivery_year.apply(lambda x: fix_delivery_year(x))\n",
    "df_test.delivery_year = df_test.delivery_year.apply(lambda x: fix_delivery_year(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Data is correct and clean ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['item_size', 'item_color', 'item_price', 'user_title', 'user_state',\n",
       "       'return', 'LOC', 'age', 'DT', 'npp', 'order_month', 'order_year',\n",
       "       'delivery_month', 'delivery_year', 'item_popularity',\n",
       "       'brand_popularity'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test = pd.get_dummies(ord_test)\n",
    "# test.columns\n",
    "ord_test = pd.DataFrame()\n",
    "ord_test['order_month'] = df_test['order_month'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hagen\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Hagen\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Hagen\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Hagen\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "### Encode categorical values\n",
    "# 1. get variables that need to be 1-hot-encoded: item_size, item_color, user_title, user_state, order_month, order_year\n",
    "# ... delivery_month, delivery_year, item_popularity, brand_popularity\n",
    "cat_train = df_train[['item_size', 'item_color', 'user_title', 'user_state',\n",
    "                     'delivery_month', 'delivery_year', 'item_popularity', 'brand_popularity']]\n",
    "\n",
    "cat_test = df_test[['item_size', 'item_color', 'user_title', 'user_state',\n",
    "                     'delivery_month', 'delivery_year', 'item_popularity', 'brand_popularity']]\n",
    "\n",
    "ord_train = pd.DataFrame()\n",
    "ord_train['order_month'] = df_train['order_month'].astype('category').copy()\n",
    "ord_train['order_year'] = df_train['order_year'].astype('category').copy()\n",
    "\n",
    "ord_test = pd.DataFrame()\n",
    "ord_test['order_month'] = df_test['order_month'].astype('category').copy()\n",
    "ord_test['order_year'] = df_test['order_year'].astype('category').copy()\n",
    "\n",
    "cat_train['order_month'] = ord_train.order_month\n",
    "cat_train['order_year'] = ord_train.order_year\n",
    "\n",
    "cat_test['order_month'] = ord_test.order_month\n",
    "cat_test['order_year'] = ord_test.order_year\n",
    "\n",
    "cat_train = pd.get_dummies(cat_train)\n",
    "cat_test = pd.get_dummies(cat_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 15)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['item_size', 'item_color', 'item_price', 'user_title', 'user_state',\n",
       "       'return', 'LOC', 'age', 'DT', 'npp', 'order_month', 'order_year',\n",
       "       'delivery_month', 'delivery_year', 'item_popularity',\n",
       "       'brand_popularity'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get continouus values\n",
    "cont_train = df_train[['item_price', 'LOC', 'age', 'DT', 'npp']].copy()\n",
    "cont_test = df_test[['item_price', 'LOC', 'age', 'DT', 'npp']].copy()\n",
    "\n",
    "#standardize them\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "sc.fit(cont_train)\n",
    "cont_train = pd.DataFrame(sc.transform(cont_train))\n",
    "cont_test = pd.DataFrame(sc.transform(cont_test))\n",
    "\n",
    "\n",
    "#join them back together\n",
    "X_train = pd.concat([cat_train, cont_train], axis=1)\n",
    "y_train = df_train['return']\n",
    "\n",
    "X_test = pd.concat([cat_test, cont_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Save X_train, y_train and X_test to a file\n",
    "X_train.to_csv('../data/X_train.csv', index=False)\n",
    "y_train.to_csv('../data/y_train.csv', index=False)\n",
    "X_test.to_csv('../data/X_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 37550\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(100000, 68)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(C=2000.0, random_state=0)\n",
    "lr.fit(X_train.values, y_train.values)\n",
    "y_predict = lr.predict(X_train.values)\n",
    "error = sum(abs(y_train.values - y_predict))\n",
    "print('Error: {}'.format(error))\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 1470\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(100000, 94)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_jobs=-1, random_state=0)\n",
    "clf.fit(X_train.values, y_train.values)\n",
    "y_predict = clf.predict(X_train.values)\n",
    "error = sum(abs(y_train.values - y_predict))\n",
    "print('Error: {}'.format(error))\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: Class dist: [46643 43356],  Acc: 0.6035396460353964\n",
      "Fold 2: Class dist: [46643 43356],  Acc: 0.6085391460853915\n",
      "Fold 3: Class dist: [46643 43356],  Acc: 0.6121387861213878\n",
      "Fold 4: Class dist: [46643 43356],  Acc: 0.5932406759324068\n",
      "Fold 5: Class dist: [46643 43357],  Acc: 0.6064\n",
      "Fold 6: Class dist: [46643 43357],  Acc: 0.6137\n",
      "Fold 7: Class dist: [46644 43357],  Acc: 0.6092609260926093\n",
      "Fold 8: Class dist: [46644 43357],  Acc: 0.605960596059606\n",
      "Fold 9: Class dist: [46644 43357],  Acc: 0.606960696069607\n",
      "Fold 10: Class dist: [46644 43357],  Acc: 0.6031603160316031\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import StratifiedKFold\n",
    "kfold = StratifiedKFold(y=y_train.values, n_folds = 10, random_state = 1)\n",
    "X_train_val = X_train.values\n",
    "X_test_val = X_test.values\n",
    "y_train_val = y_train.values\n",
    "\n",
    "scores = []\n",
    "for k, (train, test) in enumerate(kfold):\n",
    "    clf.fit(X_train_val[train], y_train_val[train])\n",
    "    score = clf.score(X_train_val[test], y_train_val[test])\n",
    "    scores.append(score)\n",
    "    print('Fold {}: Class dist: {},  Acc: {}'.format(k+1, np.bincount(y_train_val[train]), score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: Class dist: [46643 43356],  Acc: 0.6279372062793721\n",
      "Fold 2: Class dist: [46643 43356],  Acc: 0.6308369163083691\n",
      "Fold 3: Class dist: [46643 43356],  Acc: 0.6303369663033697\n",
      "Fold 4: Class dist: [46643 43356],  Acc: 0.6304369563043696\n",
      "Fold 5: Class dist: [46643 43357],  Acc: 0.6213\n",
      "Fold 6: Class dist: [46643 43357],  Acc: 0.6245\n",
      "Fold 7: Class dist: [46644 43357],  Acc: 0.6277627762776278\n",
      "Fold 8: Class dist: [46644 43357],  Acc: 0.6216621662166216\n",
      "Fold 9: Class dist: [46644 43357],  Acc: 0.6312631263126313\n",
      "Fold 10: Class dist: [46644 43357],  Acc: 0.626962696269627\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for k, (train, test) in enumerate(kfold):\n",
    "    lr.fit(X_train_val[train], y_train_val[train])\n",
    "    score = lr.score(X_train_val[test], y_train_val[test])\n",
    "    scores.append(score)\n",
    "    print('Fold {}: Class dist: {},  Acc: {}'.format(k+1, np.bincount(y_train_val[train]), score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: Class dist: [46643 43356],  Acc: 0.6279372062793721\n",
      "Fold 2: Class dist: [46643 43356],  Acc: 0.6307369263073692\n",
      "Fold 3: Class dist: [46643 43356],  Acc: 0.6304369563043696\n",
      "Fold 4: Class dist: [46643 43356],  Acc: 0.6304369563043696\n",
      "Fold 5: Class dist: [46643 43357],  Acc: 0.6213\n",
      "Fold 6: Class dist: [46643 43357],  Acc: 0.6245\n",
      "Fold 7: Class dist: [46644 43357],  Acc: 0.6277627762776278\n",
      "Fold 8: Class dist: [46644 43357],  Acc: 0.6216621662166216\n",
      "Fold 9: Class dist: [46644 43357],  Acc: 0.6312631263126313\n",
      "Fold 10: Class dist: [46644 43357],  Acc: 0.626962696269627\n"
     ]
    }
   ],
   "source": [
    "# simple pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "pipe_lr = Pipeline([['sc', StandardScaler()], ['lr', lr]])\n",
    "\n",
    "scores = []\n",
    "for k, (train, test) in enumerate(kfold):\n",
    "    pipe_lr.fit(X_train_val[train], y_train_val[train])\n",
    "    score = pipe_lr.score(X_train_val[test], y_train_val[test])\n",
    "    scores.append(score)\n",
    "    print('Fold {}: Class dist: {},  Acc: {}'.format(k+1, np.bincount(y_train_val[train]), score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC 0.679033289114622 --- Logistic Regression\n",
      "ROC AUC 0.660677857283646 --- Decision Tree\n",
      "ROC AUC 0.6651053635731221 --- Forest\n",
      "ROC AUC 0.6436364960476209 --- Bayes\n",
      "ROC AUC 0.6883574577348286 --- Majority\n"
     ]
    }
   ],
   "source": [
    "# Compare LR, against DT and Forest and  Bayes and majority_vote  \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "lr = LogisticRegression(penalty='l2', C=0.01, random_state=0)\n",
    "dt = DecisionTreeClassifier(max_depth=3, criterion='entropy', random_state=0)\n",
    "# knn = KNeighborsClassifier(n_neighbors = 3, p=2, metric='minkowski') -- \n",
    "forest = RandomForestClassifier(n_jobs=-1, random_state=0)\n",
    "bayes = GaussianNB()\n",
    "maj_vote = VotingClassifier(estimators=[('lr', lr), ('dt', dt),('bayes', bayes), ('forest', forest)], voting='soft', n_jobs=-1)\n",
    "\n",
    "clf_labels = ['Logistic Regression', 'Decision Tree', 'Forest', 'Bayes', 'Majority']\n",
    "\n",
    "for clf, label in zip([lr, dt, forest, bayes , maj_vote], clf_labels):\n",
    "    scores=cross_val_score(\n",
    "        estimator=clf,\n",
    "        X = X_train_val,\n",
    "        y = y_train_val,\n",
    "        cv = 5,\n",
    "        scoring='roc_auc'\n",
    "    )\n",
    "    print(\"ROC AUC {} --- {}\".format(scores.mean(), label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max auc_roc: 0.680980139611\n"
     ]
    }
   ],
   "source": [
    "# Tune LR\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "lr = LogisticRegression(random_state=0, n_jobs=-1)\n",
    "\n",
    "\n",
    "searchCV = LogisticRegressionCV(\n",
    "    Cs=list(np.power(10.0, np.arange(-10, 10)))\n",
    "    ,penalty='l2'\n",
    "    ,scoring='roc_auc'\n",
    "    ,cv=10\n",
    "    ,random_state=0\n",
    "    ,max_iter=10000\n",
    "    ,fit_intercept=True\n",
    "    ,solver='newton-cg'\n",
    "    ,tol=10\n",
    ")\n",
    "searchCV.fit(X_train_val, y_train_val)\n",
    "\n",
    "print ('Max auc_roc:', searchCV.scores_[1].mean(axis=0).max())\n",
    "\n",
    "# param_range = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]\n",
    "\n",
    "# param_grid = [\n",
    "#     {\n",
    "#         'C' : param_range,\n",
    "#         'penalty' : ['l1']\n",
    "#     },\n",
    "#     {\n",
    "#         'C' : param_range,\n",
    "#         'penalty' : ['l2']\n",
    "#     }\n",
    "    \n",
    "# ]\n",
    "# gs = GridSearchCV(\n",
    "#     estimator=lr,\n",
    "#     param_grid = param_grid,\n",
    "#     scoring='accuracy',\n",
    "#     cv='10',\n",
    "#     n_jobs=-1\n",
    "# )\n",
    "\n",
    "# gs.fit(X_train_val, y_train_val)\n",
    "\n",
    "# print(gs.best_score_)\n",
    "# print(gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "searchCV.C_ #use 0.01 as optimal C "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.11623724, -0.05595402, -0.02210602, -0.032771  ,  0.0142638 ,\n",
       "         0.10295247,  0.10271472, -0.13805486,  0.07888475,  0.2342548 ,\n",
       "         0.0230545 , -0.00700787,  0.04164554,  0.06954455, -0.594429  ,\n",
       "         0.07244163, -0.05907957, -0.06065697,  0.01426193, -0.05541369,\n",
       "        -0.02518371,  0.01077378,  0.25811737, -0.09284657,  0.03369121,\n",
       "         0.02312665, -0.02789765, -0.03798999,  0.01087656, -0.0517407 ,\n",
       "        -0.05820065,  0.00567408, -0.05473658, -0.03271303, -0.07998641,\n",
       "         0.17207705, -0.05804937, -0.01308426,  0.0106908 , -0.04788135,\n",
       "         0.06149866, -0.18744716, -0.08397894,  0.03303288, -0.00494813,\n",
       "         0.07477875, -0.05577817,  0.02717924, -0.11548324,  0.10505653,\n",
       "         0.04039755,  0.00751381,  0.09504469, -0.03066742,  0.13129981,\n",
       "         0.25661251,  0.34594965,  0.35765922,  0.31206555,  0.0615941 ,\n",
       "         0.05278493,  0.1449028 ,  0.0988587 ,  0.07062437, -0.24195384,\n",
       "        -1.61313872,  0.76887847,  0.7908519 , -1.61313872,  0.07369886,\n",
       "        -0.0870132 , -0.04009402,  0.02277124, -0.01739026, -0.05878933,\n",
       "         0.11821232, -0.00345596, -0.10389282, -0.10356618, -0.09455714,\n",
       "        -0.05823223,  0.04039894,  0.05062332, -0.0404124 , -0.04028801,\n",
       "        -0.02640708,  0.2081689 , -0.06427188,  0.01086353,  0.30626994,\n",
       "         0.03503626, -0.09439007, -0.04540266,  0.12925313]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "searchCV.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
